# Math for Machine Learning Resources


- [[Book](https://www.amazon.com/-/es/Thomas-Calculus-Joel-R-Hass-ebook/dp/B06XNMWWWP/ref=sr_1_1?dchild=1&keywords=thomas+calculus&qid=1589054915&sr=8-1)] Thomas' Calculus
  - Pros
    - rigorious understanding
    - getting intuition
  - Key topics:
    - Partial derivatives
    - Chain rule
    - Langrange Multipliers
    - Newton's method
    - Green's and Stoke's theorems
- [[Book](https://www.deeplearningbook.org/)] Goodfellow et al. Deep Learning
  - Pros
    - to the point
    - concise
    - learn only what you need
  - Key chapters
    - Part I2: Linear Algebra
    - Part I3: Probability and Information Theory
    - Part I4: Numerical Computation
- [[Book](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)] Bishop's Pattern Recognition and Machine Learning
  - Pros
    - to the point
    - concise
    - learn only what you need
  - Key chapters:
    - I. Introduction
    - II. Probability Distributions
    - Appendix B: Probability Distributions (very concise)
    - Appendix C: Properties of Matrics (very concise)
    - Appendix D: Calculus of Variations (very concise)
    - Appendix E: Langrange Multipliers (very concise)
- [[Youtube Channel](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)][Linear Algebra][Calculus][intuitive] 3Blue1Brown series
  - Pros:
    - very intuitive
    - amazing visualizations
  - Key series:
    - Linear Algebra series ()
    - Calculus series
- [[Coursera](drive.google.com)] Mathematics for Machine Learning Specialization
  - Pros:
    - teaches what you need
    - many ML related examples, practices
  - Key topics:
    - Linear Algebra
      - vectors + dot product + projection
      - change of basis
      - lineary dependency of set of vectors
      - vector operations
      - matrix transformations
      - Gaussian elimination
      - Finding matrix inverse by Gaussian elimination
      - Determinants and inverse
      - Matrices changing basis
      - Transformations in changing basis
      - Orthogonal Matrices
      - Gram-Schmidt process
      - Non-square matrix multiplications
      - Eigenvalues / Eigenvectors
      - Special Eigen-cases
      - Calculating Eigenvectors
      - Eigenbasis
      - Pagerank algorithm (example)
    - Multivariate Calculus
      - Variables, constants
      - Differentiate wrt anything
      - **Jacobian**
      - **Sandbit**
      - **Hessian**
      - Multivariate chain rule
      - Simple Neural Networks
      - Power Series
      - Linearilization
      - Taylor Series
      - Optimization (Gradient Descent + Constraint Optimization)
      - Regression (simple linear regression + general non-linear least squares)
    - PCA
      - Statistics of a dataset
      - Orthogonal Projections
      - PCA
